{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating A Simple News Article Web Scraper\n",
    "\n",
    "\n",
    "Large datasets for modern data applications must be generated and compiled before analysis\n",
    "can be done. A growing popularity technique for gathering large sets of data from online\n",
    "sources is web scraping. This refers to using software to gather data over time from the\n",
    "websites a developer is interested in. Depending on the amount of data needed, and the type of\n",
    "application it is needed for, web scraping might need to be done over a long period of time. In\n",
    "the case of rate limiting, some scrapers must also be scheduled at specific times of the day and\n",
    "only run for specific amounts of time. This tutorial will walk through a simple web scraper for\n",
    "news articles, setting up a database to store article data, and setting up a scheduler to run the\n",
    "scraper at specified times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use Postgresql database to save data that the scraper finds.\n",
    "\n",
    "Following are the python libraries used in the notebook\n",
    "\n",
    "    Newspaper3k (or newspaper for python 2), is a library which makes scraping news data easy\n",
    "\n",
    "    SQLAlchemy is designed to make it easy to interface with the postgres database.\n",
    "\n",
    "    APScheduler is a package that helps schedule events to happen on regular intervals, such as scraping a website every day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import newspaper\n",
    "import sqlalchemy\n",
    "import apscheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Simple Web Scraper\n",
    "\n",
    "All the necessary libraries are installed. In below code cell, \n",
    "the website URL which we want to scrape is specified. Newspaper package makes scraping easy.\n",
    "\n",
    "First, we define a url that we wish to scrape.\n",
    "\n",
    "Next, newspaper.build takes in that url and generates a newspaper object. A newspaper\n",
    "object is generated starting at the url we gave, and creates a collection of articles.\n",
    "\n",
    "The for loop prints out the title of all articles in paper.articles list. The reason for\n",
    "the if statement that checks for the presence of a title is because scraping can\n",
    "sometimes be messy, and not everything that is scraped has everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unable to cache TLDs in file /usr/lib/python3.4/site-packages/tldextract/.tld_set: [Errno 30] Read-only file system: '/usr/lib/python3.4/site-packages/tldextract/.tld_set'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Ugly' fruits and vegetables home delivery service coming to Chicago\n",
      "************\n",
      "Watchdog\n",
      "************\n",
      "Wells St. Market food hall in Loop reveals opening chef line-up\n",
      "************\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import newspaper\n",
    "paper = newspaper.build(\"http://www.chicagotribune.com/\")\n",
    "\n",
    "for article in paper.articles:\n",
    "    if article.title is not None:\n",
    "        print(article.title+\"\\n************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make postgres table for data\n",
    "\n",
    "AWS Redshift is a distributed version of postgresql. We will use a postgresql database to store the articles. Create a Redshift cluster for a postgresql database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "password········\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import random\n",
    "import time\n",
    "import json\n",
    "import psycopg2\n",
    "from getpass import getpass\n",
    "from pandas import read_sql\n",
    "import datetime\n",
    "\n",
    "\n",
    "# Create client objects for AWS redhift service. \n",
    "redshift_client = boto3.client('redshift')\n",
    "pwd = getpass('password')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the following variables, name of the cluster and a database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############### Set the following variables ##################################\n",
    "\n",
    "cluster_name=\"newsarticles\"\n",
    "\n",
    "database_name=\"articles\"\n",
    "\n",
    "Sec_group_name= \"newsArticles_Sec_group\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Create an AWS EC2 client object to create a security group for the redshift cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create client object for AWS EC2 service.\n",
    "ec2_client = boto3.client(\n",
    "    'ec2'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Create a security group. A redshift cluster is built with EC2 instances as its nodes. We need a security group while launching Redshift cluster. Get the security group Id in a variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sg = ec2_client.create_security_group(\n",
    "    Description='security group for news articles redhift cluster',\n",
    "    GroupName=Sec_group_name\n",
    ")\n",
    "Sec_group=sg[\"GroupId\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Redshift cluster listens on the port 5439. Edit the security group inbound rules to allow all TCP/IP traffic on port number 5439."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingress ALL TCP added\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    sec_rule=\"ALL TCP\"\n",
    "    data = ec2_client.authorize_security_group_ingress(\n",
    "        GroupId=Sec_group,\n",
    "        IpPermissions=[\n",
    "            {'IpProtocol': 'tcp',\n",
    "             'FromPort': 5439,\n",
    "             'ToPort': 5439,\n",
    "             'IpRanges': [{'CidrIp': '0.0.0.0/0'}]},\n",
    "        ],)\n",
    "    print(\"Ingress \"+sec_rule+\" added\")\n",
    "except:\n",
    "    print(sec_rule+\" already added\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below cell will deploy a redshift cluster. \n",
    "A default database named \"articles\" is created during the cluster deployment. \n",
    "The parameter \"NumberOfNodes\" will tell how many slave nodes the cluster should have. \n",
    "The network traffic is controlled based on the inbound rules in the security group newsArticles_Sec_group. \n",
    "At the end of the session we will delete the security group and cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "response = redshift_client.create_cluster(\n",
    "    DBName=database_name,            # Optional. A default database named dev is created for the cluster. Optionally, \n",
    "                                     # specify a custom database name (e.g. mydb) to create an additional database.\n",
    "    \n",
    "    ClusterIdentifier=cluster_name,  # Unique key that identifies a cluster. It is stored as a lowercase string. \n",
    "    ClusterType='multi-node',        # single-node is other option\n",
    "    NodeType='dc1.large',            # other options are dc1.8xlarge ds2.xlarge ds2.8xlarge ds1.xlarge ds1.8xlarge\n",
    "    MasterUsername='skaf48',     \n",
    "    MasterUserPassword=pwd,\n",
    "    ClusterSubnetGroupName='default',\n",
    "    VpcSecurityGroupIds=[\n",
    "        Sec_group,\n",
    "    ],\n",
    "    ClusterParameterGroupName='default.redshift-1.0',  # Parameter group to associate with this cluster.  \n",
    "    Port=5439,\n",
    "    AllowVersionUpgrade=True,\n",
    "    NumberOfNodes=2,   # Compute nodes store your data and execute your queries. In addition to your compute nodes, a leader \n",
    "                       # node will be added to your cluster, free of charge. The leader node is the access point for \n",
    "                       # ODBC/JDBC and generates the query plans executed on the compute nodes.\n",
    "                       # The number of nodes should be a minimum of \n",
    "    \n",
    "    PubliclyAccessible=True, # If true, cluster to be accessible from the public internet. If No, then its accessible only \n",
    "                             # from within the private VPC network\n",
    "    EnhancedVpcRouting=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below poll function keeps checking the status of cluster. \n",
    "Once it is in ready state the poll function breaks out of the loop indicating the cluster is ready to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def poll_until_completed(client, cluster_id):\n",
    "    delay = 2\n",
    "    while True:\n",
    "        # Get the cluster details\n",
    "        cluster = client.describe_clusters(ClusterIdentifier=cluster_id)\n",
    "        # Get the current status of cluster\n",
    "        status = cluster['Clusters'][0]['ClusterStatus']\n",
    "        # Get current system time \n",
    "        now = str(datetime.datetime.now().time())\n",
    "        # Print the message with the sttaus of cluster at current time\n",
    "        print(\"cluster %s is %s at %s\" % (cluster_id, status, now))\n",
    "        \n",
    "        # Below Condition keeps checking if the cluster is in available state or in final-snapshot. If yes, then break the loop\n",
    "        if status in ['available', 'final-snapshot']:\n",
    "            break\n",
    "\n",
    "        # If the cluster status is not in available or final-snapshot then wait for time and go through one more iteration.\n",
    "        delay *= random.uniform(1.1, 2.0)\n",
    "        time.sleep(delay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster newsarticles is creating at 13:28:17.906461\n",
      "cluster newsarticles is creating at 13:28:20.225458\n",
      "cluster newsarticles is creating at 13:28:23.451706\n",
      "cluster newsarticles is creating at 13:28:29.602724\n",
      "cluster newsarticles is creating at 13:28:40.009695\n",
      "cluster newsarticles is creating at 13:28:57.695425\n",
      "cluster newsarticles is creating at 13:29:20.190191\n",
      "cluster newsarticles is creating at 13:30:00.456962\n",
      "cluster newsarticles is creating at 13:31:14.647397\n",
      "cluster newsarticles is creating at 13:33:12.678090\n",
      "cluster newsarticles is available at 13:36:49.097852\n"
     ]
    }
   ],
   "source": [
    "# Wait until the cluster status changes to available. You can't use the cluster until it is available\n",
    "poll_until_completed(redshift_client, cluster_id=cluster_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To connect to the cluster we need its endpoint that is the DNS address. \n",
    "Below cell prints the end point, \n",
    "the default port(where the cluster is listening for input requests) and the database name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster endpoint: climate.cub8zvu6uo1j.us-east-1.redshift.amazonaws.com\n",
      "Port: 5439\n",
      "Database: climatecitydata\n",
      "****************************************\n",
      "Cluster endpoint: climatesdcc8b.cub8zvu6uo1j.us-east-1.redshift.amazonaws.com\n",
      "Port: 5439\n",
      "Database: climatecitydata_sdcc8b\n",
      "****************************************\n",
      "Cluster endpoint: newsarticles.cub8zvu6uo1j.us-east-1.redshift.amazonaws.com\n",
      "Port: 5439\n",
      "Database: articles\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "cluster_end_point = ''\n",
    "for cluster in redshift_client.describe_clusters()[\"Clusters\"]:\n",
    "    print(\"Cluster endpoint:\",str(cluster[\"Endpoint\"][\"Address\"])+\"\\nPort:\",str(cluster[\"Endpoint\"][\"Port\"])+\"\\nDatabase:\",str(cluster[\"DBName\"]))\n",
    "    print('*'*40)\n",
    "    cluster_end_point = str(cluster[\"Endpoint\"][\"Address\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below code cell prints the public and private addresses of the nodes in cluster.\n",
    "\n",
    "All Amazon EC2 instances are assigned two IP addresses at launch. A private IP address and a public IP address that are directly mapped to each other through Network Address Translation (NAT). Private IP addresses are only reachable from within the Amazon EC2 network. Public addresses are reachable from the Internet.\n",
    "\n",
    "Amazon EC2 also provides an internal DNS name and a public DNS name that map to the private and public IP addresses, respectively. The internal DNS name can only be resolved within Amazon EC2. The public DNS name resolves to the public IP address outside the Amazon EC2 network, and to the private IP address within the Amazon EC2 network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'NodeRole': 'COMPUTE-0', 'PrivateIPAddress': '172.31.81.100', 'PublicIPAddress': '34.226.106.73'}\n",
      "{'NodeRole': 'LEADER', 'PrivateIPAddress': '172.31.95.159', 'PublicIPAddress': '34.238.68.222'}\n",
      "{'NodeRole': 'COMPUTE-1', 'PrivateIPAddress': '172.31.93.55', 'PublicIPAddress': '34.237.248.118'}\n"
     ]
    }
   ],
   "source": [
    "for cluster in redshift_client.describe_clusters()[\"Clusters\"]:\n",
    "    for ClusterNode in cluster[\"ClusterNodes\"]:\n",
    "        if cluster_name in cluster[\"Endpoint\"][\"Address\"]:\n",
    "            print(ClusterNode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below connection string has all the credentials to connect to a AWS redshift cluster. \n",
    "It is used to connect to \"articles\" database in \"newsarticles\" cluster on port 5439."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the connection string\n",
    "conn_string = { 'dbname': database_name, \n",
    "           'user':'skaf48',\n",
    "           'pwd':pwd,\n",
    "           'host':cluster_end_point,\n",
    "           'port':'5439'\n",
    "         }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below method is establishes a connection with cluster using connect method in psycopg2 librray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_conn(config):\n",
    "    try:\n",
    "        # get a connection, if a connect cannot be made an exception will be raised here\n",
    "        con=psycopg2.connect(dbname=config['dbname'], host=config['host'], \n",
    "                              port=config['port'], user=config['user'], \n",
    "                              password=config['pwd'])\n",
    "        return con\n",
    "    except Exception as err:\n",
    "        print(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Below function call will establish the connection to the redshift cluster \"newsarticles\" using psycopg library.\n",
    "con = create_conn(config=conn_string)\n",
    "print(\"Connected to DB!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<connection object at 0x7fa1a9b9e638; dsn: 'host=newsarticles.cub8zvu6uo1j.us-east-1.redshift.amazonaws.com dbname=articles port=5439 password=xxxxxxxxxxx user=skaf48', closed: 0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The articles database is already created as default database so you can directly create a table in that database. \n",
    "Finally, the create table statement will create a table that has columns for title and body text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create table query\n",
    "statement = 'CREATE TABLE articles_table(title varchar, body varchar(max));'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cursors\n",
    "\n",
    "Rather than executing a whole query at once, it is possible to set up a cursor that encapsulates the query, and then read the query result a few rows at a time. One reason for doing this is to avoid memory overrun when the result contains a large number of rows. postgreSQL users do not normally need to worry about that, since FOR loops automatically use a cursor internally to avoid memory problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# con.cursor will return a cursor object, you can use this cursor to perform queries\n",
    "cur = con.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cur.execute(statement)\n",
    "con.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# con.rollback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column_name</th>\n",
       "      <th>data_type</th>\n",
       "      <th>character_maximum_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>body</td>\n",
       "      <td>character varying</td>\n",
       "      <td>65535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>title</td>\n",
       "      <td>character varying</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  column_name          data_type  character_maximum_length\n",
       "0        body  character varying                     65535\n",
       "1       title  character varying                       256"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# execute a Query using the con object\n",
    "\n",
    "df = read_sql(\"\"\"select column_name, data_type, character_maximum_length\n",
    "from INFORMATION_SCHEMA.COLUMNS where table_name = 'articles_table';\"\"\",con=con)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert from Scraper to Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "articles =[article for article in paper.articles  if article.title is not None]\n",
    "len(articles)\n",
    "\n",
    "for article in articles:\n",
    "    if article.title is not None:\n",
    "        \n",
    "        # download html from tree of article URLS\n",
    "        article.download()\n",
    "            \n",
    "        # parse downloaded html to readable text\n",
    "        article.parse()\n",
    "            \n",
    "        # clear newlines out of body text\n",
    "        body = article.text.replace( '\\n' , ' ' )\n",
    "        \n",
    "        try:\n",
    "            # insert into database\n",
    "            cur.execute(\"insert into articles_table values(%s,%s);\",(article.title,body))\n",
    "        except Exception as e:\n",
    "            print('database exception',e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# con.rollback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>'Ugly' fruits and vegetables home delivery ser...</td>\n",
       "      <td>Beauty is only skin deep, especially with frui...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Chicago Tribune</td>\n",
       "      <td>When Lasheena Hall opened her mail to find yet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wells St. Market food hall in Loop reveals ope...</td>\n",
       "      <td>The highly anticipated Loop food hall Wells St...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  'Ugly' fruits and vegetables home delivery ser...   \n",
       "1                                    Chicago Tribune   \n",
       "2  Wells St. Market food hall in Loop reveals ope...   \n",
       "\n",
       "                                                body  \n",
       "0  Beauty is only skin deep, especially with frui...  \n",
       "1  When Lasheena Hall opened her mail to find yet...  \n",
       "2  The highly anticipated Loop food hall Wells St...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = read_sql(\"select * from articles_table limit 5;\",con=con)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "response = redshift_client.delete_cluster(\n",
    "    ClusterIdentifier=cluster_name,\n",
    "    SkipFinalClusterSnapshot=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
