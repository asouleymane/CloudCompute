{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "\n",
    "MOst of you must be familiar with this term already. ML can be defined as a field of computer science that aims at exploring the construction of algorithms and models that can learn from data(often referred to as Ground Truth, in the case of supervised ML) help you identify patterns, and make data-driven decisions\n",
    "\n",
    "\n",
    "Following diagram is a simplified view of typical phases of machine learning process.\n",
    "\n",
    "\n",
    "<img src=\"../images/ML.PNG\">\n",
    "\n",
    "\n",
    "From a logical perspective, following is part of every Machine Learning model:\n",
    "\n",
    "- **Obtain training data** - You either build your own dataset, or locate ready-to-use publicly available data. The training data is used as as input for the training phase. You will address this in the next Lab Step. \n",
    "\n",
    "\n",
    "- **Train your model** - This is an offline phase that often requires a lot of time. This phase corresponds with the Learning Processing part of the diagram above.\n",
    "\n",
    "\n",
    "- **Store your model data** - This is usually a large matrix of numbers, based on the model complexity and the number of input features. This phase corresponds with the Modelpart of the diagram above.\n",
    "\n",
    "\n",
    "- **Evaluate your model using part of your dataset** - This is needed to verify whether your model behaves well with new data or not. The evaluation phase can be iterative, and effectively sits between the Learning Processing and Model parts of the diagram above. \n",
    "\n",
    "\n",
    "- **Deploy and use your model for real-time predictions** - By this time in the process, usually several tests have been run and evaluated based on different data sets. At this point you are narrowing it down to one and deploying it. The processing itself is often quite fast, and in some use cases even run on slow devices such as a smartphone. In this Amazon ML lab you will use AWS APIs. This phase corresponds to the Predicting Processing in the diagram above. \n",
    "\n",
    "<br>\n",
    "In this notebook, we will a create a Machine Learning model for IRIS data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Granting Amazon ML Permissions to Read Your Data from Amazon S3\n",
    "\n",
    "To create a datasource object from your input data in Amazon S3, you must grant Amazon ML the following permissions to the S3 location where your input data is stored.\n",
    "\n",
    "Use get_bucket_acl() method to know the access policies attached with the bucket. Access control lists (ACLs) are one of the resource-based access policy options that you can use to manage access to your buckets and objects. You can use ACLs to grant basic read/write permissions to other AWS accounts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"ResponseMetadata\": {\n",
      "    \"RetryAttempts\": 1,\n",
      "    \"RequestId\": \"68F5D46F4BFACD12\",\n",
      "    \"HostId\": \"38ynRKLoEXUzJ7nyks0zSULGldehYTZOWJH64qalXXIAFNOgp5dM6wIlziwftdGy7zx8TXyxv4k=\",\n",
      "    \"HTTPStatusCode\": 200,\n",
      "    \"HTTPHeaders\": {\n",
      "      \"content-type\": \"application/xml\",\n",
      "      \"server\": \"AmazonS3\",\n",
      "      \"x-amz-id-2\": \"38ynRKLoEXUzJ7nyks0zSULGldehYTZOWJH64qalXXIAFNOgp5dM6wIlziwftdGy7zx8TXyxv4k=\",\n",
      "      \"date\": \"Wed, 25 Oct 2017 23:25:19 GMT\",\n",
      "      \"x-amz-request-id\": \"68F5D46F4BFACD12\",\n",
      "      \"transfer-encoding\": \"chunked\"\n",
      "    }\n",
      "  },\n",
      "  \"Grants\": [\n",
      "    {\n",
      "      \"Permission\": \"FULL_CONTROL\",\n",
      "      \"Grantee\": {\n",
      "        \"DisplayName\": \"dsamasters\",\n",
      "        \"ID\": \"dd6b47de89624f8f1cdfe158faff7cba3652f079c0f8e9b3e8e637f300ebfe6f\",\n",
      "        \"Type\": \"CanonicalUser\"\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"Owner\": {\n",
      "    \"DisplayName\": \"dsamasters\",\n",
      "    \"ID\": \"dd6b47de89624f8f1cdfe158faff7cba3652f079c0f8e9b3e8e637f300ebfe6f\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import base64\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "import random\n",
    "\n",
    "data_s3_url = \"s3://dsabucket2/iris_data/irisdata.csv\"\n",
    "\n",
    "# Call to S3 to retrieve the policy for the given bucket\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# get_bucket_acl() method gets the access control policy for the specified bucket.\n",
    "# We want to use dsabucket2 for storing the data. Check and add more policies so AWS ML service can access the dataset\n",
    "result = s3.get_bucket_acl(Bucket='dsabucket2')\n",
    "print(json.dumps(result, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In below cell we are creating a JSON object defining the access rules who can access dsabucket2. \n",
    "\n",
    "The policy is separated into two parts because the ListBucket action requires permissions on the bucket while the other actions require permissions on the objects in the bucket. We used two different Amazon Resource Names (ARNs) to specify bucket-level and object-level permissions. The first Resource element specifies arn:aws:s3:::dsabucket2 for the ListBucket action so that applications can list all objects in the test bucket. The second Resource element specifies arn:aws:s3:::dsabucket2/* for the GetObject, PutObject, and DeletObject actions so that applications can read, write, and delete any objects in the test bucket.\n",
    "\n",
    "We did not combine the two ARNs by using a wildcard, such as arn:aws:s3:::dsabucket2*. Even though this ARN would grant permissions for all actions in a single statement, it is broader and grants access to any bucket and objects in that bucket that begin with dsabucket2, like dsabucket2-bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'HTTPHeaders': {'date': 'Wed, 25 Oct 2017 23:25:22 GMT',\n",
       "   'server': 'AmazonS3',\n",
       "   'x-amz-id-2': 'sgNsclCOsoBW5X6PoQIH1eT0maMCcrBQpeGvEDva+HzZPwfY0RqNSwWyfYH43K7LQlwLBYHUva0=',\n",
       "   'x-amz-request-id': '9E45230E0C847E50'},\n",
       "  'HTTPStatusCode': 204,\n",
       "  'HostId': 'sgNsclCOsoBW5X6PoQIH1eT0maMCcrBQpeGvEDva+HzZPwfY0RqNSwWyfYH43K7LQlwLBYHUva0=',\n",
       "  'RequestId': '9E45230E0C847E50',\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bucket_name = 'dsabucket2'\n",
    "\n",
    "bucket_policy= {\n",
    "  \"Version\": \"2012-10-17\",\n",
    "  \"Statement\": [\n",
    "    {\n",
    "      \"Effect\": \"Allow\",\n",
    "      \"Principal\": { \"Service\": \"machinelearning.amazonaws.com\"},\n",
    "      \"Action\": [\"s3:ListBucket\"],\n",
    "      \"Resource\": [\"arn:aws:s3:::dsabucket2\"]\n",
    "    },\n",
    "    {\n",
    "      \"Effect\": \"Allow\",\n",
    "      \"Principal\": { \"Service\": \"machinelearning.amazonaws.com\"},\n",
    "      \"Action\": \"s3:PutObjectAcl\",\n",
    "      \"Resource\": \"arn:aws:s3:::dsabucket2/*\"\n",
    "    },\n",
    "    {\n",
    "      \"Effect\": \"Allow\",\n",
    "      \"Principal\": { \"Service\": \"machinelearning.amazonaws.com\"},\n",
    "      \"Action\": [\n",
    "        \"s3:PutObject\",\n",
    "        \"s3:GetObject\",\n",
    "        \"s3:DeleteObject\"\n",
    "      ],\n",
    "      \"Resource\": [\"arn:aws:s3:::dsabucket2/*\"]\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "\n",
    "# Convert the policy to a JSON string\n",
    "bucket_policy = json.dumps(bucket_policy)\n",
    "\n",
    "# Set the new policy on the given bucket\n",
    "s3.put_bucket_policy(Bucket=bucket_name, Policy=bucket_policy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "s3:GetObject applies to the objects in the bucket so the Resource is correct: \"Resource\": \"arn:aws:s3:::my-bucket/*\".\n",
    "\n",
    "s3:ListBucket applies to the Bucket itself and so the Resource should be \"Resource\": \"arn:aws:s3:::my-bucket\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset selection and manipulation\n",
    "\n",
    "Data manipulation prior to using Machine Learning is very common. Below code cell has all steps to create a data source that an AWS ML model can use. Whether your input data comes from your own database, or from an open dataset, most of the time you need to manipulate the raw data. There can be several reasons for this, including:\n",
    "\n",
    "- **Data normalization (or feature scaling)** - Fortunately Amazon ML takes care of this for you, but you do want to avoid unbalanced contributions of your input features, in case their values span very different ranges.\n",
    "\n",
    "\n",
    "- **Feature selection (or subset selection)** - Once again, you don't need to worry about this when using Amazon ML, although removing redundant or irrelevant features before training your model can make it faster and more accurate.\n",
    "\n",
    "\n",
    "- **Data formatting** - Recall that models are big matrices of data. You always need to feed them with a suitable input format. In this Lab you will use a basic CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "client = boto3.client('machinelearning', region_name='us-east-1')\n",
    "\n",
    "response = client.create_data_source_from_s3(\n",
    "    DataSourceId='irisdata_dsa',\n",
    "    DataSourceName='iris data',\n",
    "    DataSpec={\n",
    "        'DataLocationS3': 's3://dsabucket2/iris_data/irisdata.csv',\n",
    "        'DataSchema': \"\"\"{ \"version\": \"1.0\", \n",
    "                         \"targetFieldName\": \"class\",\n",
    "                         \"dataFormat\": \"CSV\", \n",
    "                         \"dataFileContainsHeader\": \"true\", \n",
    "                         \"attributes\": \n",
    "                            [  \n",
    "                                { \"fieldName\": \"sepal_length\", \"fieldType\": \"NUMERIC\" }, \n",
    "                                { \"fieldName\": \"sepal_width\",  \"fieldType\": \"NUMERIC\" },  \n",
    "                                { \"fieldName\": \"petal_length\", \"fieldType\": \"NUMERIC\" }, \n",
    "                                { \"fieldName\": \"petal_width\",  \"fieldType\": \"NUMERIC\" }, \n",
    "                                { \"fieldName\": \"class\",        \"fieldType\": \"CATEGORICAL\" }\n",
    "                            ]\n",
    "                      }\"\"\"\n",
    "              },\n",
    "    ComputeStatistics=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a MLModel using the DataSource and the recipe as information sources.\n",
    "\n",
    "CreateMLModel is an asynchronous operation. In response to CreateMLModel , Amazon Machine Learning (Amazon ML) immediately returns and sets the MLModel status to PENDING .After the MLModel has been created and is ready for use, Amazon ML sets the status to COMPLETED.\n",
    "\n",
    "GetMLModel operation is used to check the progress of the MLModel during the creation operation.\n",
    "\n",
    "CreateMLModel requires a DataSource with computed statistics, which can be created by setting ComputeStatistics to true in CreateDataSourceFromS3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Below are \n",
    "schema_fn = \"iris.schema\"\n",
    "recipe_fn = \"recipe.json\"\n",
    "name = \"iris classification\"\n",
    "created_model_id=\"\"\n",
    "train_ds_id=\"\"\n",
    "test_ds_id=\"\"\n",
    "eval_id=\"\"\n",
    "\n",
    "access_id='AKIAILNFGTHCYJ54GSXA'\n",
    "secret_key='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'\n",
    "\n",
    "aws_access_key_id=access_id\n",
    "aws_secret_access_key=secret_key\n",
    "\n",
    "\n",
    "# create AWS machine learning clinet\n",
    "ml = boto3.client('machinelearning', region_name='us-east-1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model(data_s3_url, schema_fn, recipe_fn, name, train_percent=70):\n",
    "    \"\"\"Creates all the objects needed to build an ML Model & evaluate its quality.\n",
    "    \"\"\"\n",
    "    # ml - aws machine learning object\n",
    "    # data_s3_url - S3 location where input data is located\n",
    "    # schema_fn - schema of IRIS dataset\n",
    "    # train_percent - How much % data you want in training set\n",
    "    # name - name of the ML model\n",
    "    # recipe_fn - Its a JSON file that describes the attributes in the dataset\n",
    "    \n",
    "    # Create train and test data sources\n",
    "    (train_ds_id, test_ds_id) = create_data_sources(ml, data_s3_url, schema_fn, train_percent, name)\n",
    "    \n",
    "    # Create the model using train dataset\n",
    "    ml_model_id = create_model(ml, train_ds_id, recipe_fn, name)\n",
    "    \n",
    "    eval_id = create_evaluation(ml, ml_model_id, test_ds_id, name)\n",
    "\n",
    "    return ml_model_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "\n",
    "def create_data_sources(ml, data_s3_url, schema_fn, train_percent, name):\n",
    "    \"\"\"Create two data sources.  One with (train_percent)% of the data,\n",
    "    which will be used for training.  The other one with the remainder of the data,\n",
    "    which is commonly called the \"test set\" and will be used to evaluate the quality\n",
    "    of the ML Model.\n",
    "    \"\"\"\n",
    "    train_ds_id = 'ds-' + str(uuid.uuid4())\n",
    "    spec = {\n",
    "        \"DataLocationS3\": data_s3_url,\n",
    "        \"DataRearrangement\": json.dumps({\n",
    "            \"splitting\": {\n",
    "                \"percentBegin\": 0,\n",
    "                \"percentEnd\": train_percent,\n",
    "                \"strategy\":\"random\"\n",
    "            },\n",
    "        }),\n",
    "        \"DataSchema\": open(schema_fn).read(),\n",
    "    }\n",
    "    \n",
    "    ml.create_data_source_from_s3(\n",
    "        DataSourceId=train_ds_id,\n",
    "        DataSpec=spec,\n",
    "        DataSourceName=name + \" - training split\",\n",
    "        ComputeStatistics=True\n",
    "    )    \n",
    "    \n",
    "    print(\"Created training data set %s\" % train_ds_id)\n",
    "\n",
    "    test_ds_id = 'ds-' + str(uuid.uuid4())\n",
    "    spec['DataRearrangement'] = json.dumps({\n",
    "        \"splitting\": {\n",
    "            \"percentBegin\": train_percent,\n",
    "            \"percentEnd\": 100,\n",
    "            \"strategy\":\"random\"\n",
    "        }\n",
    "    })\n",
    "    ml.create_data_source_from_s3(\n",
    "        DataSourceId=test_ds_id,\n",
    "        DataSpec=spec,\n",
    "        DataSourceName=name + \" - testing split\",\n",
    "        ComputeStatistics=True\n",
    "    )\n",
    "    print(\"Created test data set %s\" % test_ds_id)\n",
    "    return (train_ds_id, test_ds_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are calling create_ml_model() method inside create_model() method definition for creating a new MLModel using the DataSource and the recipe as information sources.\n",
    "\n",
    "\n",
    "Syntax: \n",
    "\n",
    "    response = client.create_ml_model(\n",
    "        MLModelId='string',\n",
    "        MLModelName='string',\n",
    "        MLModelType='REGRESSION'|'BINARY'|'MULTICLASS',\n",
    "        Parameters={\n",
    "            'string': 'string'\n",
    "        },\n",
    "        TrainingDataSourceId='string',\n",
    "        Recipe='string',\n",
    "        RecipeUri='string'\n",
    "    )\n",
    "    \n",
    "\n",
    "\n",
    "- **sgd.maxMLModelSizeInBytes** - The maximum allowed size of the model. Depending on the input data, the size of the model might affect its performance. The value is an integer that ranges from 100000 to 2147483648 . The default value is 33554432 .\n",
    "\n",
    "\n",
    "- **sgd.maxPasses** - The number of times that the training process traverses the observations to build the MLModel . The value is an integer that ranges from 1 to 10000 . The default value is 10 .\n",
    "\n",
    "\n",
    "- **sgd.l1RegularizationAmount** - The coefficient regularization L1 norm. It controls overfitting the data by penalizing large coefficients. This tends to drive coefficients to zero, resulting in a sparse feature set. If you use this parameter, start by specifying a small value, such as 1.0E-08 . The value is a double that ranges from 0 to MAX_DOUBLE . The default is to not use L1 normalization. This parameter can't be used when L2 is specified. Use this parameter sparingly.\n",
    "\n",
    "\n",
    "- **Recipe (string)** -- The data recipe for creating the MLModel . You must specify either the recipe or its URI. If you don't specify a recipe or its URI, Amazon ML creates a default.\n",
    "\n",
    "\n",
    "- **RecipeUri (string)** -- The Amazon Simple Storage Service (Amazon S3) location and file name that contains the MLModel recipe. You must specify either the recipe or its URI. If you don't specify a recipe or its URI, Amazon ML creates a default.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model(ml, train_ds_id, recipe_fn, name):\n",
    "    \"\"\"Creates an ML Model object, which begins the training process.\n",
    "The quality of the model that the training algorithm produces depends\n",
    "primarily on the data, but also on the hyper-parameters specified\n",
    "in the parameters map, and the feature-processing recipe.\n",
    "    \"\"\"\n",
    "    created_model_id = 'ml-' + str(uuid.uuid4())\n",
    "    ml.create_ml_model(\n",
    "        MLModelId=created_model_id,\n",
    "        MLModelName=name + \" model\",\n",
    "        MLModelType=\"MULTICLASS\",  # we're predicting True/False values\n",
    "        Parameters={\n",
    "            # Refer to the \"Machine Learning Concepts\" documentation\n",
    "            # for guidelines on tuning your model\n",
    "            \"sgd.maxPasses\": \"100\",\n",
    "            \"sgd.maxMLModelSizeInBytes\": \"104857600\",  # 100 MiB\n",
    "            \"sgd.l2RegularizationAmount\": \"1e-4\",\n",
    "        },\n",
    "        Recipe=open(recipe_fn).read(),\n",
    "        TrainingDataSourceId=train_ds_id\n",
    "    )\n",
    "    print(\"Created ML Model %s\" % created_model_id)\n",
    "    return created_model_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_evaluation(ml, model_id, test_ds_id, name):\n",
    "    eval_id = 'ev-' + str(uuid.uuid4())\n",
    "    ml.create_evaluation(\n",
    "        EvaluationId=eval_id,\n",
    "        EvaluationName=name + \" evaluation\",\n",
    "        MLModelId=model_id,\n",
    "        EvaluationDataSourceId=test_ds_id\n",
    "    )\n",
    "    print(\"Created Evaluation %s\" % eval_id)\n",
    "    return eval_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the model\n",
    "\n",
    "<a id='creating_model'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created training data set ds-64a1a487-3326-4e38-a780-3757a7234cba\n",
      "Created test data set ds-4f78c84d-c7f3-4a75-8695-487e4d7bbd79\n",
      "Created ML Model ml-e5aaa2d2-e8cd-403f-9c76-64bd7db98ccd\n",
      "Created Evaluation ev-b267bb01-c3e1-45da-8438-9dc993c2e289\n"
     ]
    }
   ],
   "source": [
    "model_id = build_model(data_s3_url, schema_fn, recipe_fn, name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have created the model but it takes some time for the model to be built and ready to use. Below poll function will keep polling until the status changes to anything under ['COMPLETED', 'FAILED', 'INVALID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def poll_until_completed(ml, model_id):\n",
    "    delay = 2\n",
    "    while True:\n",
    "        model = ml.get_ml_model(MLModelId=model_id)\n",
    "        status = model['Status']\n",
    "        message = model.get('Message', '')\n",
    "        now = str(datetime.datetime.now().time())\n",
    "        print(\"Model %s is %s (%s) at %s\" % (model_id, status, message, now))\n",
    "        if status in ['COMPLETED', 'FAILED', 'INVALID']:\n",
    "            break\n",
    "\n",
    "        # exponential backoff with jitter\n",
    "        delay *= random.uniform(1.1, 2.0)\n",
    "        time.sleep(delay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model ml-e5aaa2d2-e8cd-403f-9c76-64bd7db98ccd is PENDING () at 18:25:52.228068\n",
      "Model ml-e5aaa2d2-e8cd-403f-9c76-64bd7db98ccd is PENDING () at 18:25:54.501489\n",
      "Model ml-e5aaa2d2-e8cd-403f-9c76-64bd7db98ccd is PENDING () at 18:25:58.557017\n",
      "Model ml-e5aaa2d2-e8cd-403f-9c76-64bd7db98ccd is PENDING () at 18:26:04.783355\n",
      "Model ml-e5aaa2d2-e8cd-403f-9c76-64bd7db98ccd is PENDING () at 18:26:11.680060\n",
      "Model ml-e5aaa2d2-e8cd-403f-9c76-64bd7db98ccd is PENDING () at 18:26:20.623765\n",
      "Model ml-e5aaa2d2-e8cd-403f-9c76-64bd7db98ccd is PENDING () at 18:26:36.417128\n",
      "Model ml-e5aaa2d2-e8cd-403f-9c76-64bd7db98ccd is PENDING () at 18:26:54.296305\n",
      "Model ml-e5aaa2d2-e8cd-403f-9c76-64bd7db98ccd is PENDING () at 18:27:29.596144\n",
      "Model ml-e5aaa2d2-e8cd-403f-9c76-64bd7db98ccd is PENDING () at 18:28:09.839975\n",
      "Model ml-e5aaa2d2-e8cd-403f-9c76-64bd7db98ccd is PENDING () at 18:29:06.563653\n",
      "Model ml-e5aaa2d2-e8cd-403f-9c76-64bd7db98ccd is INPROGRESS () at 18:30:37.168024\n",
      "Model ml-e5aaa2d2-e8cd-403f-9c76-64bd7db98ccd is COMPLETED () at 18:33:15.183476\n"
     ]
    }
   ],
   "source": [
    "poll_until_completed(ml, model_id=model_id)  # Can't use it until it's COMPLETED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait until the model is created. Once it is in completed status, you can run create_realtime_endpoint() function in below code cell to create end point for the model for making realtime predictions on new test data. The endpoint contains the URI of the MLModel. That is, the location to send real-time prediction requests for the specified MLModel.\n",
    "\n",
    "## Note:\n",
    "Make sure to check and update the model id in below cell with the latest machine learning model we created above to create the end point. \n",
    "\n",
    "[Go to this cell to make sure the ML id is same](#creating_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.create_realtime_endpoint(\n",
    "    MLModelId=model_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MLModelId': 'ml-e5aaa2d2-e8cd-403f-9c76-64bd7db98ccd',\n",
      " 'RealtimeEndpointInfo': {'CreatedAt': datetime.datetime(2017, 10, 25, 18, 36, 33, 568000, tzinfo=tzlocal()),\n",
      "                          'EndpointStatus': 'UPDATING',\n",
      "                          'EndpointUrl': 'https://realtime.machinelearning.us-east-1.amazonaws.com',\n",
      "                          'PeakRequestsPerSecond': 0},\n",
      " 'ResponseMetadata': {'HTTPHeaders': {'content-length': '236',\n",
      "                                      'content-type': 'application/x-amz-json-1.1',\n",
      "                                      'date': 'Wed, 25 Oct 2017 23:36:33 '\n",
      "                                              'GMT',\n",
      "                                      'x-amzn-requestid': '55bc1610-b9dd-11e7-aff9-117e4c814d57'},\n",
      "                      'HTTPStatusCode': 200,\n",
      "                      'RequestId': '55bc1610-b9dd-11e7-aff9-117e4c814d57',\n",
      "                      'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "pprint.pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation:\n",
    "\n",
    "Lets evaluate the model by checking the predictive ability. The test record present in [iris_record.csv](iris_record.csv)(csv file is present in current working directory /module3/labs/iris_recrd.csv) has one record whose species is virginica. Lets test the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait for a minute before running below cell. The create_realtime_endpoint() method in above cell will create an endpoint for a making predictions and it will take some time in updating the status of the endpoint.\n",
    "\n",
    "## Note:\n",
    "\n",
    "Make sure the end point in below cell matches with the endpoint given in the response output of create_realtime_endpoint() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"ResponseMetadata\": {\n",
      "    \"RetryAttempts\": 0,\n",
      "    \"RequestId\": \"8ac76c52-b9df-11e7-a28c-4323e2c51fd4\",\n",
      "    \"HTTPStatusCode\": 200,\n",
      "    \"HTTPHeaders\": {\n",
      "      \"x-amzn-requestid\": \"8ac76c52-b9df-11e7-a28c-4323e2c51fd4\",\n",
      "      \"content-type\": \"application/x-amz-json-1.1\",\n",
      "      \"date\": \"Wed, 25 Oct 2017 23:52:21 GMT\",\n",
      "      \"content-length\": \"223\"\n",
      "    }\n",
      "  },\n",
      "  \"Prediction\": {\n",
      "    \"predictedScores\": {\n",
      "      \"setosa\": 0.17459960281848907,\n",
      "      \"versicolor\": 0.24952912330627441,\n",
      "      \"virginica\": 0.5758712887763977\n",
      "    },\n",
      "    \"details\": {\n",
      "      \"PredictiveModelType\": \"MULTICLASS\",\n",
      "      \"Algorithm\": \"SGD\"\n",
      "    },\n",
      "    \"predictedLabel\": \"virginica\"\n",
      "  }\n",
      "}\n",
      "******************************\n",
      "Its a virginica.\n"
     ]
    }
   ],
   "source": [
    "# from boto3.session import Session\n",
    "import json\n",
    " \n",
    "# session = Session(aws_access_key_id=access_id, aws_secret_access_key=secret_key)\n",
    " \n",
    "try:\n",
    "    model = ml.get_ml_model(MLModelId=model_id)\n",
    "    prediction_endpoint = 'https://realtime.machinelearning.us-east-1.amazonaws.com'\n",
    " \n",
    "    with open('iris_record.csv') as f:\n",
    "        record_str = f.readline()\n",
    " \n",
    "    record = {}\n",
    "    for index,val in enumerate(record_str.split(',')):\n",
    "        record['Var%03d' % (index+1)] = val\n",
    " \n",
    "    response = ml.predict(MLModelId=model_id, Record=record, PredictEndpoint=prediction_endpoint)\n",
    "    print(json.dumps(response, indent=2))\n",
    "    label = response.get('Prediction').get('predictedLabel')\n",
    "    print(\"*\"*30)\n",
    "    print(\"Its a %s.\" % label)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It did predict the leaf belongs to the species Virginica. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Predictions\n",
    "\n",
    "Below code cells demonstrate how to use an ML Model, to kick off a batch prediction job, which uses the ML Model to generate predictions on new data. It takes the ML Model and test data to make the predictions. create_batch_prediction() method writes the prediction results to the supplied S3 location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The URL of the sample data in S3\n",
    "\n",
    "def use_model(ml, model_id, output_s3, inputdatasource_id):\n",
    "    \"\"\"Creates all the objects needed to build an ML Model & evaluate its quality.\n",
    "    \"\"\"\n",
    "\n",
    "    poll_until_completed(ml, model_id)  # Can't use it until it's COMPLETED\n",
    "#     ml.update_ml_model(MLModelId=model_id, ScoreThreshold=threshold)\n",
    "    print(\"Set score threshold for %s to %.2f\" % (model_id, threshold))\n",
    "\n",
    "    bp_id = 'bp-' + str(uuid.uuid4())\n",
    "    ml.create_batch_prediction(\n",
    "        BatchPredictionId=bp_id,\n",
    "        BatchPredictionName=\"Batch Prediction for marketing sample\",\n",
    "        MLModelId=model_id,\n",
    "        BatchPredictionDataSourceId=inputdatasource_id,\n",
    "        OutputUri=output_s3\n",
    "    )\n",
    "    print(\"Created Batch Prediction %s\" % bp_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the method use_model(), to make the batch predictions. Results will be written to the location \"s3://dsabucket2/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model ml-e5aaa2d2-e8cd-403f-9c76-64bd7db98ccd is COMPLETED () at 19:03:25.720506\n",
      "Set score threshold for ml-e5aaa2d2-e8cd-403f-9c76-64bd7db98ccd to 0.70\n",
      "Created Batch Prediction bp-bc699687-b50c-4501-a77a-e2c66d1a6586\n"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "import boto3\n",
    "import datetime\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "import urllib\n",
    "\n",
    "threshold = 0.7\n",
    "s3_output_url = \"s3://dsabucket2/\"\n",
    "UNSCORED_DATA_ID = \"ds-6ee0a76b-b6a6-42e5-bd71-1b60b9bec057\"\n",
    "# parsed_url = urlparse.parse(s3_output_url)\n",
    "\n",
    "use_model(ml, model_id, s3_output_url, UNSCORED_DATA_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List the objects in the S3 bucket \"s3://dsabucket2/\" to get the address of the results directory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/stackoverflow/stackoverflow_survey.csv\n",
      "\n",
      "batch-prediction/bp-335b03ff-fbad-4949-9123-b9564dd0094b.manifest\n",
      "\n",
      "batch-prediction/result/bp-335b03ff-fbad-4949-9123-b9564dd0094b-irisdata.csv.gz\n",
      "\n",
      "iris_data/irisdata.csv\n",
      "\n",
      "redshift_logs/_dummy\n",
      "\n",
      "redshift_logs/df-019246711DHET93EIDE3/Ec2Instance/@Ec2Instance_2017-10-22T15:21:56/@Ec2Instance_2017-10-22T15:21:56_Attempt=1/TaskRunner.2017-10-22-15@000000000000000-000000000033313.gz\n",
      "\n",
      "redshift_logs/df-019246711DHET93EIDE3/RedshiftLoadActivity/@RedshiftLoadActivity_2017-10-22T15:21:56/@RedshiftLoadActivity_2017-10-22T15:21:56_Attempt=1/Activity.log.gz\n",
      "\n",
      "redshift_logs/df-019246711DHET93EIDE3/RedshiftLoadActivity/@RedshiftLoadActivity_2017-10-22T15:21:56/@RedshiftLoadActivity_2017-10-22T15:21:56_Attempt=2/Activity.log.gz\n",
      "\n",
      "redshift_logs/df-019246711DHET93EIDE3/RedshiftLoadActivity/@RedshiftLoadActivity_2017-10-22T15:21:56/@RedshiftLoadActivity_2017-10-22T15:21:56_Attempt=3/Activity.log.gz\n",
      "\n",
      "stackoverflow/stackoverflow_survey.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bucket='dsabucket2'\n",
    "# s3 = boto3.client('s3')  # again assumes boto.cfg setup, assume AWS S3\n",
    "for obj in s3.list_objects(Bucket=bucket)['Contents']:\n",
    "    print(obj['Key']+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The batch prediction results are stored in the location \"dsabucket2/batch-prediction/result/\". The results are saved as a zip file. Take the zip file, convert it in to a stream of bytes. GzipFile() takes the compresssed file and decompresses it. The final output is in the form of strings. \n",
    "\n",
    "\n",
    "## Note:\n",
    "\n",
    "Fill the blank for the location of prediction results file name.  \n",
    "\n",
    "It looks similar to below\n",
    "\n",
    "    batch-prediction/result/bp-335b03ff-fbad-4949-9123-b9564dd0094b-irisdata.csv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from io import BytesIO\n",
    "from gzip import GzipFile\n",
    "\n",
    "retr = s3.get_object(Bucket=bucket, Key='batch-prediction/result/<>') # Get the file name from above list objects output\n",
    "bytestream = BytesIO(retr['Body'].read())\n",
    "got_text = GzipFile(None, 'rb', fileobj=bytestream).read().decode('utf-8')\n",
    "type(got_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the string output extracted in above cell into a csv file. These are the predictions from the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "s = io.StringIO(got_text)\n",
    "with open('iris_results.csv', 'w') as f:\n",
    "    for line in s:\n",
    "        f.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the csv file into a pandas dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  trueLabel    setosa  versicolor  virginica\n",
      "0    setosa  0.999532    0.000201   0.000267\n",
      "1    setosa  0.996606    0.002008   0.001386\n",
      "2    setosa  0.966665    0.016052   0.017282\n",
      "3    setosa  0.998670    0.000660   0.000670\n",
      "4    setosa  0.992480    0.002195   0.005325\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "with open('iris_results.csv', 'r') as file:\n",
    "    df = pd.read_csv(file)\n",
    "#     df.reset_index(inplace=True)\n",
    "    print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model has a column for true labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    setosa\n",
       "1    setosa\n",
       "2    setosa\n",
       "3    setosa\n",
       "4    setosa\n",
       "Name: trueLabel, dtype: object"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "truelabels=df['trueLabel']\n",
    "truelabels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df.idxmax() function expects numerical inputs. The \"trueLabel\" df is of type string. Since we alreday captured true labels in the variable truelabels, delete the column trueLabel from the dataframe \"df\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del df['trueLabel']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probabilities are generated for different classes in the form of predictions. These probabilities indicate the chance of the row belonging to certain species. Use these probabilities to label the predictions. In the next code cell, we are using df.idxmax() function which returns the column name which has the highest value of all rows. \n",
    "\n",
    "Now predictions have the species name predicted for each of the 50 rows in test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=df.idxmax(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a confusion matrix using trueLabels and predicted labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[13,  0,  0],\n",
       "       [ 0,  9,  4],\n",
       "       [ 0,  0, 16]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(truelabels, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.904761904762\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "print(metrics.accuracy_score(truelabels, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There it is. The model built by AWS machine learning service predicted the species with 90% accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting predictions for the HAR data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from boto3.session import Session\n",
    "# import json\n",
    " \n",
    "# session = Session(aws_access_key_id='AKIAILNFGTHCYJ54GSXA', aws_secret_access_key='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')\n",
    "# machinelearning = session.client('machinelearning', region_name='us-east-1')\n",
    "# model_id = 'ml-Q9T8QNWIWBR'\n",
    "# labels = {'1': 'walking', '2': 'walking upstairs', '3': 'walking downstairs', '4': 'sitting', '5': 'standing', '6': 'laying'}\n",
    " \n",
    "# try:\n",
    "#     model = machinelearning.get_ml_model(MLModelId=model_id)\n",
    "#     prediction_endpoint = 'https://realtime.machinelearning.us-east-1.amazonaws.com'\n",
    " \n",
    "#     with open('record.csv') as f:\n",
    "#         record_str = f.readline()\n",
    " \n",
    "#     record = {}\n",
    "#     for index,val in enumerate(record_str.split(',')):\n",
    "#         record['Var%03d' % (index+1)] = val\n",
    " \n",
    "#     response = machinelearning.predict(MLModelId=model_id, Record=record, PredictEndpoint=prediction_endpoint)\n",
    "#     print(json.dumps(response, indent=2))\n",
    "#     label = response.get('Prediction').get('predictedLabel')\n",
    "#     print(\"*\"*30)\n",
    "#     print(\"You are currently %s.\" % labels[label])\n",
    "    \n",
    "# except Exception as e:\n",
    "#     print(e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
