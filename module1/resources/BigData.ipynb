{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Big Data\n",
    "\n",
    "When you say Big data we are looking at data sets so large and so complex that it's not possible to process them on one computer. \n",
    "They are impossible to process using the traditional databases and tools that have existed for the last 40 years in computer science. \n",
    "\n",
    "Hadoop, distributed processing, map-reduce programming are some of the words that pop into someone's mind when they think of big data with an assumption that its the ideal choice when dealing with large datasets. \n",
    "Lets look at a simple example which is discussed in Database Analytics course. \n",
    "We will compare the time taken by a traditional PostgreSQL database and Apache drill + Hadoop "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "### Pulling a few rows from a large table\n",
    "\n",
    "#### PostgreSQL using the psycopg interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import psycopg2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# The typical Connection String format for PostgreSQL\n",
    "connect_str = \"dbname='twitter' user='dsa_ro_user' host='dbase.dsa.missouri.edu'password='readonly'\"\n",
    "\n",
    "with psycopg2.connect(connect_str) as conn:\n",
    "\n",
    "    # Accumulate total time and list\n",
    "    total_time = 0.0\n",
    "    times = []\n",
    "    \n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    ############### START Timing Iterations\n",
    "    for n in range(0,35):\n",
    "        \n",
    "        # Start the Timer\n",
    "        start = time.perf_counter()\n",
    "\n",
    "        # This is a very basic query, \n",
    "        # fetch any 500 rows from a table of over 100 million\n",
    "        cursor.execute(\"SELECT * FROM twitter.hashtag LIMIT 500\")\n",
    "\n",
    "        # This pulls all the result rows in the Python Memory\n",
    "        cols = cursor.fetchall()\n",
    "\n",
    "        end = time.perf_counter()\n",
    "\n",
    "        # Accumulate and list\n",
    "        total_time += (end - start)\n",
    "        times.append(end - start)\n",
    "\n",
    "    ############### END Timing Iterations\n",
    "        \n",
    "    # How long did this look up take, typically?\n",
    "    print('Time Fetch Rows   ')\n",
    "    print('------------------')\n",
    "    print(\"Median Fetch Time: {0:.5f}\".format(np.median(times)))\n",
    "    print(\"Total Fetch Time: {0:.5f}\".format(np.sum(times)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hadoop + Drill using the PyDrill interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from pydrill.client import PyDrill\n",
    "\n",
    "drill = PyDrill(host='rvip.cgi.missouri.edu', port=8047)\n",
    "\n",
    "if not drill.is_active():\n",
    "    raise ImproperlyConfigured('Please run Drill first')\n",
    "\n",
    "############### START Timing Iterations\n",
    "for n in range(0,35):\n",
    "\n",
    "    # Start the Timer\n",
    "    start = time.perf_counter()\n",
    "\n",
    "    # This is a very basic query, \n",
    "    # fetch any 500 rows from a table of over 100 million\n",
    "    # This pulls all the result rows in the Python Memory\n",
    "    rows = drill.query(\"SELECT * FROM  dfs.datasets.`twitter/hashtag.json` LIMIT 500\")\n",
    "\n",
    "    end = time.perf_counter()\n",
    "\n",
    "    # Accumulate and list\n",
    "    total_time += (end - start)\n",
    "    times.append(end - start)\n",
    "\n",
    "############### END Timing Iterations\n",
    "\n",
    "# How long did this look up take, typically?\n",
    "print('Time Fetch Rows   ')\n",
    "print('------------------')\n",
    "print(\"Median Fetch Time: {0:.5f}\".format(np.median(times)))\n",
    "print(\"Total Fetch Time: {0:.5f}\".format(np.sum(times)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Talking about infrastructure, \n",
    "\n",
    "#### The 16-core PostgreSQL server is out performing the 280-core Hadoop cluster?\n",
    "\n",
    "##### Yes, and to add to that ... \n",
    "<span style=\"background:yellow\">\n",
    "PostgreSQL does not parallelize within-query execution.</span>\n",
    "**Those PG times are from a single core!**\n",
    "\n",
    "This is an important concept that is lost on many people that get caught up in the hype of \"Big Data Ecosystems\".\n",
    "\n",
    "** Big Data $\\neq$ High-Performance **\n",
    "\n",
    "All the big data ecosystems built on top of Hadoop will lose this test, \n",
    "becase the tasks is aligned against the assumptions and technical decisions that went in to developing Hadoop.\n",
    "\n",
    "**The technical why?**\n",
    "\n",
    "This particular query gets no acceleration from \"divide and conquer\" strategy.\n",
    "Recall that Hadoop does not support local cache of file blocks, partially due to the massive _block size_ (64MB) versus a tradition block size (4KB - 8KB).\n",
    "PostgreSQL on the otherhand was designed and built using *systems programming* techniques, \n",
    "leveraging _interprocess communication_ technology, specifically Shared Memory Segments in this case.\n",
    "The PostgreSQL DB caches each _page file_ (the on-disk partion blocks of table rows) it reads from disk into shared memory segments (RAM).\n",
    "Therefore, running the same query 100 or a million times in fast succession on static data has \n",
    "only one single filesystem read phase.\n",
    "After the first run of the SQL, PostgreSQL knows that it does not need all the data, \n",
    "only a small set (in this case) of page files.\n",
    "These are cached in the Shared Memory Allocation, \n",
    "and therefore accessible to any next processes or the same process that needs that data or executes that same query.\n",
    "\n",
    "In contrast, all 10 nodes of the Hadoop system must read the data each time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Large Distributed Processing\n",
    "So, Hadoop is designed for large distributed data processing that addresses every file in the database. \n",
    "And that type of processing takes time. \n",
    "For tasks like running end-of-day reports to review daily transactions, scanning historical data where performance/speed is not critical Hadoop is ideal.\n",
    "\n",
    "On the other hand, in cases where organizations rely on time-sensitive data analysis,\n",
    "a traditional database is the better fit. \n",
    "Hadoop does so well when it is needed to analyze large unstructured datasets. \n",
    "Traditional databases are well equipped to analyze smaller data sets in real time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of the size and complexity nature of Big Data it is hard to capture, store, copy, share, search, analyze, visualize, delete etc.\n",
    "There are many challenges dealing with big data. \n",
    "But it lets us gain insights and competitive edge by leveraging mountains of valuable data.\n",
    "Big Data represents the information assets that are characterized by high volume, \n",
    "high velocity of data of different varierties. \n",
    "By applying different computational and analytical methods, \n",
    "this vast data gets transformed into something worthwhile for society and businesses. \n",
    "\n",
    "\n",
    "For example, internet of things, social networks, heavy machinery like telescopes, \n",
    "Medical data, genomics all are producing huge amounts of information. \n",
    "What do we do with this data? <span style=\"background:yellow\">Analytics</span>. \n",
    "What we're looking for is, predictive analytics, so you can do forecasts and projections. \n",
    "Find connections and correlations to discover hidden patterns or maybe discover a new physics law \n",
    "or find a gene pattern which is responsible for a certain disease.\n",
    "So where's the future?\n",
    "We cannot do predictive analytics on vast data on a single computer.\n",
    "We need to put that into a cloud to do that. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
