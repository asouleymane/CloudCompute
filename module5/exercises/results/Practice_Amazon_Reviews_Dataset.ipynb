{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Amazon Reviews Data exploration\n",
    "\n",
    "\n",
    "In this exercises notebook, you will be working with amazon reviews dataset hosted on AWS S3. The dataset is already downloaded during the deployment of EMR cluster. Read the data into an Sql dataframe and explore the data. \n",
    "\n",
    "\n",
    "Create an SQLContext instance. You need before you can do anything in this notebook. To create a basic instance, all we need is a SparkContext reference. Since we are running Spark in shell mode (using pySpark) we can use the global context object sc for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataframe in PySpark: Overview\n",
    "\n",
    "In Apache Spark, a DataFrame is a distributed collection of rows under named columns. In simple terms, it is same as a table in relational database or an Excel sheet with Column headers. It also shares some common characteristics with RDD:\n",
    "\n",
    "**Immutable in nature** : We can create DataFrame / RDD once but can’t change it. And we can transform a DataFrame / RDD  after applying transformations.\n",
    "\n",
    "\n",
    "**Lazy Evaluations**: Which means that a task is not executed until an action is performed.\n",
    "\n",
    "\n",
    "**Distributed**: RDD and DataFrame both are distributed in nature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading From json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = sqlContext.read.json(\"/user/hadoop/Datasets/reviews.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's view the schema of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- helpfuless_count: long (nullable = true)\n",
      " |-- helpfuless_score: long (nullable = true)\n",
      " |-- price: string (nullable = true)\n",
      " |-- productId: string (nullable = true)\n",
      " |-- profileName: string (nullable = true)\n",
      " |-- score: string (nullable = true)\n",
      " |-- summary: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- time: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For getting the columns name we can use columns on DataFrame, similar to what we do for getting the columns in pandas DataFrame. Let’s first print the number of columns and columns name  in train file then in test file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['helpfuless_count', 'helpfuless_score', 'price', 'productId', 'profileName', 'score', 'summary', 'text', 'time', 'title', 'userId']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(df.columns))\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the Data\n",
    "\n",
    "Our data is now loaded into a dataframe that we named df, with all the dtypes inferred. First we'll count the number of rows it found:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34686770"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we look at the column-by-column dtypes the system estimated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('helpfuless_count', 'bigint'), ('helpfuless_score', 'bigint'), ('price', 'string'), ('productId', 'string'), ('profileName', 'string'), ('score', 'string'), ('summary', 'string'), ('text', 'string'), ('time', 'string'), ('title', 'string'), ('userId', 'string')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each pairing (a tuple object in Python, denoted by the parentheses), the first entry is the column name and the second is the dtype.\n",
    "\n",
    "Take a peak at five rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(helpfuless_count=7, helpfuless_score=7, price=u' unknown', productId=u' B000179R3I', profileName=u' Jeanmarie Kabala \"JP Kabala\"', score=u' 4.0', summary=u' Periwinkle Dartmouth Blazer', text=u' I own the Austin Reed dartmouth blazer in every color in which they make it-- it is a staple of my business wardrobe. Well made, quality fabric, nicely tailored, classic lines, appropriate for a professional woman. (something that can be hard to find at times) It should be noted, however, that the periwinkle and raspberry colors are lovely, but the fabric and buttons are slightly different than the \"classic\" colors(lighter) and the linings and interfacings are not as substantial as the brown, navy, camel, red and ivory. It\\'s still a good value, particularly as these are colors appropriate to warmer seasons and climates, but I was a bit surprised.', time=u' 1182816000', title=u' Amazon.com', userId=u' A3Q0VJTUO4EZ56'), Row(helpfuless_count=0, helpfuless_score=0, price=u' 17.99', productId=u' B000GKXY34', profileName=u' M. Gingras', score=u' 5.0', summary=u' Great fun!', text=u' Got these last Christmas as a gag gift. They are great fun, but obviously this is not a toy that lasts!', time=u' 1262304000', title=u' Nun Chuck, Novelty Nun Toss Toy', userId=u' ADX8VLDUOL7BG')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the format column_name=value for each row. Note that the formatting above is ugly because take doesn't try to make it pretty, it just returns the row object itself. We can use show instead and that attempts to format the data better, but because there are so many columns in this case the formatting of show doesn't fit, and each line wraps down to the next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------------+--------+-----------+--------------------+-----+--------------------+--------------------+-----------+--------------------+---------------+\n",
      "|helpfuless_count|helpfuless_score|   price|  productId|         profileName|score|             summary|                text|       time|               title|         userId|\n",
      "+----------------+----------------+--------+-----------+--------------------+-----+--------------------+--------------------+-----------+--------------------+---------------+\n",
      "|               7|               7| unknown| B000179R3I| Jeanmarie Kabala...|  4.0| Periwinkle Dartm...| I own the Austin...| 1182816000|          Amazon.com| A3Q0VJTUO4EZ56|\n",
      "|               0|               0|   17.99| B000GKXY34|          M. Gingras|  5.0|          Great fun!| Got these last C...| 1262304000| Nun Chuck, Novel...|  ADX8VLDUOL7BG|\n",
      "+----------------+----------------+--------+-----------+--------------------+-----+--------------------+--------------------+-----------+--------------------+---------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(2,truncate= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Describe\n",
    "\n",
    "Now we'll describe the data. Note that describe returns a new dataframe with the information, and so must have show called after it if our goal is to view it (note the nice formatting in this case). This can be called on one or more specific columns, as we do here, or the entire dataframe by passing no columns to describe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------------+------------------+\n",
      "|summary| helpfuless_count|  helpfuless_score|             price|\n",
      "+-------+-----------------+------------------+------------------+\n",
      "|  count|         34686770|          34686770|          34686770|\n",
      "|   mean|5.451863030198545|3.7175363113948054|25.637601311864604|\n",
      "| stddev|22.40765220943555| 19.94850094783318| 49.64204197419118|\n",
      "|    min|                0|                 0|              0.00|\n",
      "|    max|            48334|             47516|           unknown|\n",
      "+-------+-----------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_described = df.describe('helpfuless_count', 'helpfuless_score', 'price')\n",
    "df_described.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subsetting by Columns\n",
    "\n",
    "One of the simplest subsettings is done by selecting just a few of the columns:\n",
    "\n",
    "To subset the columns, we need to use select operation on DataFrame and we need to pass the columns names separated by commas inside select Operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n",
      "|         userId|score|\n",
      "+---------------+-----+\n",
      "| A3Q0VJTUO4EZ56|  4.0|\n",
      "|  ADX8VLDUOL7BG|  5.0|\n",
      "| A3NM6P6BIWTIAE|  3.0|\n",
      "|  AVCGYZL8FQQTD|  4.0|\n",
      "|        unknown|  5.0|\n",
      "+---------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Name: org.apache.toree.interpreter.broker.BrokerException\n",
       "Message: Traceback (most recent call last):\n",
       "  File \"/tmp/kernel-PySpark-649f2882-e923-4609-a12b-0edf11f9c1e1/pyspark_runner.py\", line 174, in <module>\n",
       "    eval(compiled_code)\n",
       "  File \"<string>\", line 3, in <module>\n",
       "AttributeError: 'NoneType' object has no attribute 'show'\n",
       "\n",
       "StackTrace: org.apache.toree.interpreter.broker.BrokerState$$anonfun$markFailure$1.apply(BrokerState.scala:158)\n",
       "org.apache.toree.interpreter.broker.BrokerState$$anonfun$markFailure$1.apply(BrokerState.scala:158)\n",
       "scala.Option.foreach(Option.scala:236)\n",
       "org.apache.toree.interpreter.broker.BrokerState.markFailure(BrokerState.scala:157)\n",
       "sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n",
       "sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "java.lang.reflect.Method.invoke(Method.java:606)\n",
       "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n",
       "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\n",
       "py4j.Gateway.invoke(Gateway.java:259)\n",
       "py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n",
       "py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "py4j.GatewayConnection.run(GatewayConnection.java:209)\n",
       "java.lang.Thread.run(Thread.java:748)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df_select = df.select(col('userId'),col('score')).show(5)\n",
    "df_select.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that show defaults to showing the first 20 rows, but here we've specified only 5. There is also a shortcut for this notation that does the same thing but is a little easier to read. We show both because they both show up frequently in Spark resources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n",
      "|         userId|score|\n",
      "+---------------+-----+\n",
      "| A3Q0VJTUO4EZ56|  4.0|\n",
      "|  ADX8VLDUOL7BG|  5.0|\n",
      "| A3NM6P6BIWTIAE|  3.0|\n",
      "|  AVCGYZL8FQQTD|  4.0|\n",
      "|        unknown|  5.0|\n",
      "+---------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_select = df[['userId', 'score']]\n",
    "df_select.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can do the same thing by dropping, which is convenient if we want to keep more columns than we want to drop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_drop = df_select.drop(col('score'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|         userId|\n",
      "+---------------+\n",
      "| A3Q0VJTUO4EZ56|\n",
      "|  ADX8VLDUOL7BG|\n",
      "| A3NM6P6BIWTIAE|\n",
      "|  AVCGYZL8FQQTD|\n",
      "|        unknown|\n",
      "+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_drop.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subsetting by Rows\n",
    "\n",
    "We often want to subset by rows also, for example by specifying a conditional. Note that we have to use .show() at the end of .describe(), because .describe() returns a new dataframe with the information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|             score|\n",
      "+-------+------------------+\n",
      "|  count|          34686770|\n",
      "|   mean| 4.172705011161316|\n",
      "| stddev|1.2528687360830613|\n",
      "|    min|               1.0|\n",
      "|    max|               5.0|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe('score').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_sub = df.where(df['score'] < 4.172)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------------+--------+-----------+--------------------+-----+--------------------+--------------------+-----------+--------------------+---------------+\n",
      "|helpfuless_count|helpfuless_score|   price|  productId|         profileName|score|             summary|                text|       time|               title|         userId|\n",
      "+----------------+----------------+--------+-----------+--------------------+-----+--------------------+--------------------+-----------+--------------------+---------------+\n",
      "|               7|               7| unknown| B000179R3I| Jeanmarie Kabala...|  4.0| Periwinkle Dartm...| I own the Austin...| 1182816000|          Amazon.com| A3Q0VJTUO4EZ56|\n",
      "|               1|               0|   17.99| B000GKXY34|     Maria Carpenter|  3.0|  more like funchuck| Gave this to my ...| 1224633600| Nun Chuck, Novel...| A3NM6P6BIWTIAE|\n",
      "|               7|               7| unknown| 1882931173| Jim of Oz \"jim-o...|  4.0| Nice collection ...| This is only for...|  940636800| Its Only Art If ...|  AVCGYZL8FQQTD|\n",
      "|               3|               2| unknown| B000058A81|             highter|  4.0| Stuning even for...| This sound track...|  985392000|        Chrono Cross| A3CKAD2MAEM157|\n",
      "|               3|               1| unknown| B000058A81| Stephen E. Schaefer|  4.0|    Chrono Cross OST| The music of Yas...| 1020297600|        Chrono Cross| A27NEM07T7CDQ4|\n",
      "+----------------+----------------+--------+-----------+--------------------+-----+--------------------+--------------------+-----------+--------------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sub.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can repeat the same procedure for multiple conditions and columns using standard logical operators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_filter = df.where((df['score'] > 1) & (df['score'] < 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------------+--------+-----------+----------------+-----+--------------------+--------------------+-----------+--------------------+---------------+\n",
      "|helpfuless_count|helpfuless_score|   price|  productId|     profileName|score|             summary|                text|       time|               title|         userId|\n",
      "+----------------+----------------+--------+-----------+----------------+-----+--------------------+--------------------+-----------+--------------------+---------------+\n",
      "|               1|               0|   17.99| B000GKXY34| Maria Carpenter|  3.0|  more like funchuck| Gave this to my ...| 1224633600| Nun Chuck, Novel...| A3NM6P6BIWTIAE|\n",
      "|               1|               1|   46.34| B000278ADA|     Trina Wehle|  3.0| Delivery was ver...| It took almost 3...| 1352505600| Jobst Ultrasheer...|  A9Q3932GX4FX8|\n",
      "|               1|               1|   46.34| B000278ADA|          dgodoy|  2.0| sizes recomended...| sizes are much s...| 1287014400| Jobst Ultrasheer...|  AUIZ1GNBTG5OB|\n",
      "|               0|               0| unknown| B0007FIF28|         lisamac|  3.0|            Overbury| Full of intrigue...| 1313366400| The Overbury aff...| A2GERYVE64DIPL|\n",
      "|               0|               0| unknown| B00002066E|         unknown|  3.0| Better than I th...| I wrote a harsh ...|  991180800|         Roy Orbison|        unknown|\n",
      "+----------------+----------------+--------+-----------+----------------+-----+--------------------+--------------------+-----------+--------------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_filter.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Sampling\n",
    "\n",
    "And finally, you might want to take a random sample of rows. This can be particularlly useful, for example, if your data is large enough to require more expensive clusters to be spun up to work with it all, and you want to use a smaller, less expensive cluster to work on a sample. Once your code is completed, you can then spin up the more expensive cluster and simply apply your code to the full sample.\n",
    "\n",
    "You can pass three arguments into sample: **the first is a boolean, which is True to sample with replacement, False without**. The second is the **fraction of the dataset** to take, in this case 5%, and the third is an **optional random seed**. If you specify any integer here then someone else performing the same random operation that specifies the same seed will get the same result. If no seed is passed then the exact random sampling can't be duplicated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_sample = df.sample(False, 0.05, 99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|             score|\n",
      "+-------+------------------+\n",
      "|  count|           1732754|\n",
      "|   mean| 4.171809731791125|\n",
      "| stddev|1.2531445587944696|\n",
      "|    min|               1.0|\n",
      "|    max|               5.0|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sample.describe('score').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you compare this to our original summary stats on unfiltered column userId, you'll see it does a pretty good job maintaining the mean and stddev in a sample of only 5% of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counting the distinct rows\n",
    "The distinct operation can be used here, to calculate the number of distinct rows in a DataFrame. Let’s apply distinct operation to calculate the number of distinct products in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2441053"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select('productId').distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group by a coulmn\n",
    "\n",
    "Here is how you can group the data by a column. We used the \"userId\" column to group the reviews data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n",
      "|         userId|count|\n",
      "+---------------+-----+\n",
      "| A16FTKP8BXKGHG|    1|\n",
      "|  A2H06FLSHCGNX|    2|\n",
      "| A29Q0CLOF0U8BN|    5|\n",
      "| A38N9A0UJVYIRI|    1|\n",
      "| A13ZCL7UXEDF14|  226|\n",
      "| A1OAFHNE81JHOM|    1|\n",
      "| A17UU8GM4NSMEJ|    6|\n",
      "| A2GI02PFIHFYPK|   14|\n",
      "|  AVRVACFKJ5QMM|  113|\n",
      "| A290DASWU6TETY|    8|\n",
      "|  ADIB6IP2IWMT4|   37|\n",
      "| A2E695LJSYIX98|    4|\n",
      "| A1YHAR3SQE33UA|    4|\n",
      "| A1E2LXUERE1QDX|    2|\n",
      "|  AY4OROA8ZNYH8|    1|\n",
      "| A2BMM11E76H9WC|   59|\n",
      "| A1VPXCFO261VWH|  351|\n",
      "| A2PXFI7VDHQ7BU|   61|\n",
      "| A3RV69EPSMI2XI|   60|\n",
      "| A1OXF8GW1MUJI5|   60|\n",
      "+---------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"userId\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running SQL Queries Programmatically\n",
    "\n",
    "The sql function on a SQLContext enables applications to run SQL queries programmatically and returns the result as a DataFrame. We use registerTempTable() to register the \"Reviews\" RDD as a table. The lifetime of this temporary table is tied to the SQLContext that was used to create this DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Registers the Reviews RDD as a table.\n",
    "df.registerTempTable(\"Reviews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|   count|\n",
      "+--------+\n",
      "|34686770|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = sqlContext.sql(\"SELECT count(*) as count FROM Reviews\")\n",
    "\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+\n",
      "|              userId| _c1|\n",
      "+--------------------+----+\n",
      "| A00747652TYK4MJF...| 5.0|\n",
      "| A01069451H5M05XW...| 5.0|\n",
      "| A0331603K6Z5GSVD...| 5.0|\n",
      "| A0434254310BU34Z...| 5.0|\n",
      "| A04465292GUIUWGT...| 5.0|\n",
      "| A05600701HJDE30L...| 4.0|\n",
      "| A05616363SN1G36M...| 1.0|\n",
      "| A0595191WNUKZV8KM3G| 5.0|\n",
      "| A0707282Z8AJG3NR...| 5.0|\n",
      "| A07532193EC6QULM...| 5.0|\n",
      "| A08672373PELMP72...| 4.0|\n",
      "|      A1000WA98BFTQB| 1.0|\n",
      "|      A100I83J5W64FH| 5.0|\n",
      "|       A100QWNGRXC3S| 5.0|\n",
      "|      A100WTPUCJMZXR| 3.0|\n",
      "|      A100ZMU8VBXEZ3| 5.0|\n",
      "|      A10103MJIKKIFE| 5.0|\n",
      "| A10127132IE1A73I...| 5.0|\n",
      "|      A101592SF9EIWR| 4.0|\n",
      "|      A10182LLGKCHDH| 5.0|\n",
      "+--------------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = sqlContext.sql(\"SELECT userId, max(score) FROM Reviews group by userId\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# You are done\n",
    "\n",
    "Please go back to the EMR_Deploy notebook and run the remaining code cells to clode the cluster."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
