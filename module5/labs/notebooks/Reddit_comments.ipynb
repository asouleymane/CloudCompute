{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Apache Spark\n",
    "\n",
    "Apache Spark is a framework for distributed computing; this framework aims to\n",
    "make it simpler to write programs that run in parallel across many nodes in a cluster\n",
    "of computers.\n",
    "\n",
    "It is designed from the ground up for high performance in applications of \n",
    "iterative nature, where the same data is accessed multiple times. This performance is\n",
    "achieved primarily through caching datasets in memory, combined with low latency\n",
    "and overhead to launch parallel computation tasks. Together with other features\n",
    "such as fault tolerance, flexible distributed-memory data structures, and a powerful\n",
    "functional API, Spark has proved to be broadly useful for a wide range of large-scale\n",
    "data processing tasks, over and above machine learning and iterative analytics.\n",
    "\n",
    "A Spark cluster is made up of two types of processes: a driver program and multiple\n",
    "executors. In the local mode, all these processes are run within the same JVM. In a\n",
    "cluster, these processes are usually run on separate nodes.\n",
    "\n",
    "\n",
    "Components\n",
    "Spark applications run as independent sets of processes on a cluster, coordinated by the SparkContext\n",
    "object in your main program (called the driver program).\n",
    "Specifically, to run on a cluster, the SparkContext can connect to several types of cluster managers (either\n",
    "Spark’s own standalone cluster manager, Mesos or YARN), which allocate resources across applications.\n",
    "Once connected, Spark acquires executors on nodes in the cluster, which are processes that run\n",
    "computations and store data for your application. Next, it sends your application code (defined by JAR or\n",
    "Python files passed to SparkContext) to the executors. Finally, SparkContext sends tasks to the executors to\n",
    "run.\n",
    "\n",
    "\n",
    "<img src=\"../../images/spark_components.PNG\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SparkContext and SparkConf\n",
    "\n",
    "The starting point of writing any Spark program is SparkContext (or\n",
    "JavaSparkContext in Java). SparkContext is initialized with an instance of a\n",
    "SparkConf object, which contains various Spark cluster-configuration settings (for\n",
    "example, the URL of the master node). \n",
    "\n",
    "Once initialized, we will use the various methods found in the SparkContext object\n",
    "to create and manipulate distributed datasets and shared variables. The Spark shell\n",
    "(in both Scala and Python, which is unfortunately not supported in Java) takes care\n",
    "of this context initialization for us. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resilient Distributed Datasets\n",
    "\n",
    "The core of Spark is a concept called the Resilient Distributed Dataset (RDD).\n",
    "An RDD is a collection of \"records\" (strictly speaking, objects of some type) that is\n",
    "distributed or partitioned across many nodes in a cluster (for the purposes of the\n",
    "Spark local mode, the single multithreaded process can be thought of in the same\n",
    "way). An RDD in Spark is fault-tolerant; this means that if a given node or task fails\n",
    "(for some reason other than erroneous user code, such as hardware failure, loss of\n",
    "communication, and so on), the RDD can be reconstructed automatically on the\n",
    "remaining nodes and the job will still complete.\n",
    "\n",
    "\n",
    "### Creating RDDs\n",
    "\n",
    "RDDs can be created from existing collections, for example, in a Scala Spark shell type as below:\n",
    "    \n",
    "    val collection = List(\"a\", \"b\", \"c\", \"d\", \"e\")\n",
    "    val rddFromCollection = sc.parallelize(collection)\n",
    "    \n",
    "RDDs can also be created from Hadoop-based input sources, including the local\n",
    "filesystem, HDFS, and Amazon S3. The following\n",
    "code is an example of creating an RDD from a text file located on the local filesystem:\n",
    "    \n",
    "    val rddFromTextFile = sc.textFile(\"LICENSE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Read this documentation](http://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-overview.html) on AWS website for an overview on AWS EMR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark operations\n",
    "\n",
    "Once we have created an RDD, we have a distributed collection of records that\n",
    "we can manipulate. In Spark's programming model, operations are split into\n",
    "transformations and actions. Generally speaking, a transformation operation applies\n",
    "some function to all the records in the dataset, changing the records in some way.\n",
    "An action typically runs some computation or aggregation operation and returns the\n",
    "result to the driver program where SparkContext is running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.context.SparkContext object at 0x7f6b05536e10>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It provide access to many of the underlying structures used by pySpark.\n",
    "\n",
    "The entry point into all SQL functionality in Spark is the SQLContext class. To create a basic instance, all we need is a SparkContext reference. Since we are running Spark in shell mode (using pySpark) we can use the global context object sc for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ll then create an RDD using sc.parallelize with 20 partitions which will be distributed amongst the Spark Worker nodes and also verify the number of partitions in the RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize(range(1000), 20) \n",
    "rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take a look at the first five records using the take action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s now perform a few transformations on the RDD which will bin the numbers into the lowest 100s and count the total frequency for each bin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.0, 100), (800.0, 100), (900.0, 100), (100.0, 100), (200.0, 100), (300.0, 100), (400.0, 100), (500.0, 100), (600.0, 100), (700.0, 100)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.map(lambda r: (round(r/100)*100, 1))\\\n",
    "  .reduceByKey(lambda x,y: x+y)\\\n",
    "  .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with Amazon S3, DataFrames and Spark SQL\n",
    "\n",
    "Let’s now try to read some data from Amazon S3 using the Spark SQL Context. This makes parsing JSON files significantly easier than before. After the reading the parsed data in, the resulting output is a Spark DataFrame. We can then register this as a table and run SQL queries off of it for simple analytics.\n",
    "The data we will be working with is a subset of the reddit comments published and compiled by reddit user /u/Stuck_In_the_Matrix, in r/datasets. The current example is processing reddit comments collected in May 2015 which is roughly 30GB.\n",
    "\n",
    "\n",
    "In this example we will calculate the number of distinct gilded authors and the average score of all the comments in each subreddit for the month of May, 2015. The results will then be ranked by the number of distinct gilded authors per subreddit and the average score of all the comments per subreddit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now read in all the comments from May, 2015 using the Spark SQL Context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fields = [StructField(\"archived\", BooleanType(), True), \n",
    "          StructField(\"author\", StringType(), True), \n",
    "          StructField(\"author_flair_css_class\", StringType(), True), \n",
    "          StructField(\"body\", StringType(), True), \n",
    "          StructField(\"controversiality\", LongType(), True), \n",
    "          StructField(\"created_utc\", StringType(), True), \n",
    "          StructField(\"distinguished\", StringType(), True), \n",
    "          StructField(\"downs\", LongType(), True), \n",
    "          StructField(\"edited\", StringType(), True), \n",
    "          StructField(\"gilded\", LongType(), True), \n",
    "          StructField(\"id\", StringType(), True), \n",
    "          StructField(\"link_id\", StringType(), True), \n",
    "          StructField(\"name\", StringType(), True), \n",
    "          StructField(\"parent_id\", StringType(), True), \n",
    "          StructField(\"retrieved_on\", LongType(), True), \n",
    "          StructField(\"score\", LongType(), True), \n",
    "          StructField(\"score_hidden\", BooleanType(), True), \n",
    "          StructField(\"subreddit\", StringType(), True), \n",
    "          StructField(\"subreddit_id\", StringType(), True), \n",
    "          StructField(\"ups\", LongType(), True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rawDF = sqlContext.read.json(\"s3n://reddit-comments/2015/RC_2015-05\", StructType(fields)).persist(StorageLevel.MEMORY_AND_DISK_SER).registerTempTable(\"comments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are first defining the schema of the JSON file. Not defining this is also an option; however, Spark will then need to pass through the data twice to:\n",
    "    \n",
    "    infer the schema\n",
    "\n",
    "    parse the data into a Spark DataFrame\n",
    "    \n",
    "----\n",
    "\n",
    "This can be very time consuming when datasets grow much larger. Since we know what the schema will be for this static dataset, it is in our best interest to define it beforehand. Allowing Spark to infer the schema is particularly useful, however, for scenarios when schemas change over time and fields are added or removed.\n",
    "\n",
    "\n",
    "Next the data is read from the public S3 reddit-comments bucket as a Spark DataFrame using <span style=\"color:#a5541a\"><b>sqlContext.read.json(\"...\")</b></span>. Manipulations on the Spark DataFrame in most cases are significantly more efficient that working with the core RDDs.\n",
    "\n",
    "\n",
    "After reading in the data, we would also like to persist it into memory and disk for multiple uses later on with <span style=\"color:#a5541a\"><b>.persist(StorageLevel.MEMORY_AND_DISK_SER)</b></span>. Choosing the memory and disk option permits Spark to gracefully spill the data to disk if it is too large for memory across all the Spark Worker nodes. \n",
    "\n",
    "Here we will be executing two queries on the dataset. The second query will be able to read directly from the persisted data instead of having to read in the entire dataset again.\n",
    "\n",
    "\n",
    "Lastly the Spark DataFrame is registered as a table with <span style=\"color:#a5541a\"><b>.registerTempTable(\"comments\")</b></span>, so we can run SQL queries off of it. The table can then be referenced by the name \"comments\".\n",
    "\n",
    "\n",
    "Let’s now run some SQL queries on the dataset to find the total number of distinct gilded authors and the average comment score per subreddit for this month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "distinct_gilded_authors_by_subreddit = sqlContext.sql(\"\"\" \n",
    "    SELECT subreddit, COUNT(DISTINCT author) as authors \n",
    "    FROM comments \n",
    "    WHERE gilded > 0 \n",
    "    GROUP BY subreddit \n",
    "    ORDER BY authors DESC \n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "average_score_by_subreddit = sqlContext.sql(\"\"\" \n",
    "    SELECT subreddit, AVG(score) as avg_score \n",
    "    FROM comments \n",
    "    GROUP BY subreddit \n",
    "    ORDER BY avg_score DESC \n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s take a look at the top 5 subreddits with the most gilded authors commenting and highest average comment score. Note that every command until now has been a transformation and no data has actually flowed through this point. We have essentially been building a Directed Acyclic Graph (DAG) for the operations to perform on the data. Data only begins flowing through when an action is called such as .collect(), .take(), .first(), etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(subreddit=u'AskReddit', authors=2677), Row(subreddit=u'funny', authors=506), Row(subreddit=u'pics', authors=459), Row(subreddit=u'videos', authors=379), Row(subreddit=u'news', authors=355)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distinct_gilded_authors_by_subreddit.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is the first action taken, all the 30GB will be read in and parsed from S3. This should take about 15 minutes depending on the region of Spark cluster.\n",
    "\n",
    "\n",
    "You will notice that the next action finishes in about 30 seconds. This is because Spark knows that the original data is persisted into memory and disk and does not need to go to S3 to get the data. Had we not persisted the data at the very beginning, this action would take another 15 minutes (30X slower)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(subreddit=u'karlsruhe', avg_score=73.3157894736842), Row(subreddit=u'picturesofiansleeping', avg_score=22.92391304347826), Row(subreddit=u'photoshopbattles', avg_score=21.04499959532738), Row(subreddit=u'behindthegifs', avg_score=20.62438118811881), Row(subreddit=u'IAmA', avg_score=18.381243801552937)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_score_by_subreddit.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# You are done"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
